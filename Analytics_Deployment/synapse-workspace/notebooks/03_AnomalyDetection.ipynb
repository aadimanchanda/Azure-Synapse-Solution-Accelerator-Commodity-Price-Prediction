{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Copyright (c) Microsoft Corporation. \r\n",
        "Licensed under the MIT license. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Anomaly Detection\n",
        "Load data and and apply Spark anomaly detection model:\n",
        "\n",
        "1. Define variables\n",
        "2. Load data\n",
        "3. Get access key to cognitive service\n",
        "4. Apply anomaly detection model\n",
        "5. Store results\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import mmlspark\n",
        "\n",
        "if mmlspark.__spark_package_version__ < \"1.0.0-rc3\":\n",
        "    raise Exception(\"This notebook is not compatible with the current version of mmlspark: {}. Please upgrade to 1.0.0-rc3 or higher.\".format(\n",
        "        mmlspark.__spark_package_version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from mmlspark.cognitive import *\n",
        "from notebookutils import mssparkutils\n",
        "from pyspark.sql.functions import col, lit\n",
        "from pyspark.sql.types import DoubleType"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define variables\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "# Azure storage access info\n",
        "data_lake_account_name = \"\"\n",
        "file_system_name = \"\"\n",
        "\n",
        "\n",
        "# Allow SPARK to read from Blob remotely\n",
        "adls_path = \"abfss://%s@%s.dfs.core.windows.net/CommodityAggrData\" % (file_system_name, data_lake_account_name)\n",
        "scored_data_path = \"abfss://%s@%s.dfs.core.windows.net/Result/Anomalies\" % (file_system_name, data_lake_account_name)\n",
        "\n",
        "# Cognitive Services credentials\n",
        "\n",
        "# Azure Key Vault Linked Service name \n",
        "linked_service = \"\" \n",
        "# Azure Key Vault name \n",
        "akv_name = \"\" \n",
        "# Azure Key Vault Secret name\n",
        "secret_name = \"\"  \n",
        "# Azure Cognitive Service Region \n",
        "anomaly_location = \"\" "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load data\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Load the data into a Spark DataFrame\n",
        "df = spark.read.parquet(adls_path)\n",
        "\n",
        "df = df.withColumn('date',col('date').cast('string'))\n",
        "df = df.withColumn('group',lit('serie1'))\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieve Access Keys\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Fetch the subscription key (or a general Cognitive Service key) from Azure Key Vault\n",
        "service_key = mssparkutils.credentials.getSecret(\n",
        "    linkedService=linked_service,\n",
        "    akvName=akv_name, \n",
        "    secret=secret_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instantiate and apply model\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Instantiate anamoly detector\n",
        "anomalyDetector = (SimpleDetectAnomalies()\n",
        "    .setLocation(anomaly_location)\n",
        "    .setSubscriptionKey(service_key)\n",
        "    .setOutputCol(\"output\")\n",
        "    .setErrorCol(\"error\")\n",
        "    .setGranularity(\"monthly\")\n",
        "    .setTimestampCol(\"date\")\n",
        "    .setValueCol(\"average_value\")\n",
        "    .setGroupbyCol(\"group\")\n",
        "    )\n",
        "\n",
        "# Apply anamoly detector\n",
        "df_results = anomalyDetector.transform(df)\n",
        "display(df_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Store results\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "#Store results \n",
        "df_results = df_results.select('date','average_value','output.*')\n",
        "df_results = df_results.withColumn('upperValue', col('expectedValue')+col('upperMargin')).withColumn('lowerValue', col('expectedValue')+col('lowerMargin'))\n",
        "df_results.write.mode('overwrite').save(scored_data_path,format='parquet')\n",
        "df_results.write.mode(\"overwrite\").saveAsTable('Anomalies') "
      ]
    }
  ]
}