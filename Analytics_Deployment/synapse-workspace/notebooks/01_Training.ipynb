{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright (c) Microsoft Corporation.\n",
        "Licensed under the MIT license."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training\n",
        "Pre-process data and use the data to build an Azure AutoML model in this notebook using the following steps:\n",
        "\n",
        "1. Define variables\n",
        "2. Load data and setup Azure Machine learning (AML) connection\n",
        "3. Train-test split\n",
        "4. Train and Azure automl model\n",
        "5. Register model to AML\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "from azureml.core import Experiment, Workspace, Dataset, Datastore\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "from azureml.data.dataset_factory import TabularDatasetFactory"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Variables\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Dataset name and AzureML experiment setup\n",
        "subscription_id = \"\"\n",
        "resource_group = \"\"\n",
        "workspace_name = \"\"\n",
        "experiment_name = \"commodity-price-forecast\"\n",
        "data_lake_account_name = \"\"\n",
        "file_system_name = \"\"\n",
        "table_test_name = \"default.test\"\n",
        "\n",
        "# Default train-test split on year\n",
        "split_year = 2019"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup AML and Load Data\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Azure Storage path\n",
        "adls_path = \"abfss://%s@%s.dfs.core.windows.net/CommodityAggrData\" % (file_system_name, data_lake_account_name)\n",
        "\n",
        "# Connect to Workspace\n",
        "ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
        "\n",
        "# Setup training experiment\n",
        "experiment = Experiment(ws, experiment_name)\n",
        "\n",
        "# Get datastore\n",
        "datastore = Datastore.get_default(ws)\n",
        "\n",
        "# Load Data\n",
        "df = spark.read.parquet(adls_path)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Test split\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Train-test split\n",
        "df_train = df.filter(f\"Year(date) < {split_year}\")\n",
        "df_test = df.filter(f\"Year(date) >= {split_year}\")\n",
        "\n",
        "# Register dataset to AML datastore\n",
        "dataset = TabularDatasetFactory.register_spark_dataframe(df_train, datastore, name = experiment_name + \"-dataset\")\n",
        "\n",
        "# Store test dataset for prediction\n",
        "df_test.write.mode(\"overwrite\").saveAsTable(table_test_name) "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define and train Automl model\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.automl.core.forecasting_parameters import ForecastingParameters\n",
        "\n",
        "# Set automl forecasting parameters\n",
        "forecasting_parameters = ForecastingParameters(\n",
        "    time_column_name = \"Date\",\n",
        "    forecast_horizon = 12,\n",
        ")\n",
        "\n",
        "# Automl config\n",
        "automl_config = AutoMLConfig(spark_context = sc,\n",
        "                             task = \"forecasting\",\n",
        "                             training_data = dataset,\n",
        "                             label_column_name = \"average_value\",\n",
        "                             primary_metric = \"normalized_mean_absolute_error\",\n",
        "                             experiment_timeout_hours = 0.25,\n",
        "                             max_concurrent_iterations = 2,\n",
        "                             n_cross_validations = 5,\n",
        "                             forecasting_parameters = forecasting_parameters)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Run experiment\n",
        "run = experiment.submit(automl_config)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the experiment is running, we can view the run in the AML workspace. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show experiment URL\n",
        "displayHTML(\"<a href={} target='_blank'>Your experiment in Azure Machine Learning portal: {}</a>\".format(run.get_portal_url(), run.id))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register Model\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "run.wait_for_completion()\n",
        "\n",
        "# Install required dependency\n",
        "import pip\n",
        "pip.main([\"install\", \"azure-storage-blob==12.5.0\"])\n",
        "\n",
        "import mlflow\n",
        "\n",
        "# Get best model from automl run\n",
        "best_run, non_onnx_model = run.get_output()\n",
        "\n",
        "artifact_path = experiment_name + \"_artifact\"\n",
        "\n",
        "# Config mflow to monitor results to AML\n",
        "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "with mlflow.start_run() as run:\n",
        "    # Save the model to the outputs directory for capture\n",
        "    mlflow.sklearn.log_model(non_onnx_model, artifact_path)\n",
        "\n",
        "    # Register the model to AML model registry\n",
        "    mlflow.register_model(\"runs:/\" + run.info.run_id + \"/\" + artifact_path, \"commodity-price-forecast-Best\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "language": "python"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}